\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\title{CROAC: Counting and Recognition using Omnidirectional Acoustic Capture}
\author{Marcel Gietzmann-Sanders}
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{The Pond}
It's an hour past sunset and the rain has just subsided. I approach the pond - a vernal pool that will dry up sometime this summer - trying as best I can to be a ghost. Unfortunately the crinkling of leaves reveals my corporeality and a hush descends upon the pond. It is as if the frogs recognize that their rehearsal time is over and have gone backstage in a mist of whispers as they let me get seated in anticipation of the grand event. Groping about in the darkness, as I have put out my head torch, I find the most comfortable seat in the house - a patch of dirt nestled among the roots of an old maple tree - and settle in. On the walk of mile or so in I've already heard the spring peepers chirping in all their eagerness and bullfrogs announcing their grandiose dominion with their earthly croaks, but I know that what all but a taste of what is to come. So I settle back  and wait for the concert to start.

It begins with solitary croaks as the bolder of the frogs begin to test the air, seeing whether their audience still stirs. But soon enough their comrades join in and the sound begins to mount. Layer by layer I hear different species enter the ensemble. Region by region the pond gets louder and louder as the frogs regain their composure. Before I know it the air is so thick with sound I feel as if I could breath it in. 

In full chorus it is impossible for me to discern the individual voices that make up this extraordinary orchestra. Instead all my two ears receive is a wall of frenetic sound. Yet the physicist in me recognizes this as a mere illusion and as I sit there bathed in chirps and croaks and wheezes I relish the richness of the data before me. I know that what my ears hear as a single curtain is in fact a richly woven fabric of pitches, amplitudes, and phases - a mathematical puzzle waiting to be untangled and solved. I realize that every spring night, and many a summer one too, these frogs broadcast into the ether a whole host of information on position, counts, energy, species, and perhaps even lineage. All waiting to be deciphered by a discerning soul. And so as I sat there at the base of that maple tree, bathed in the ciphered data of my amphibian friends, I wondered what it would take to break the code of frogs. The adventure into mathematics, computation, and herpetology that ensued has brought me untold joy, and I hope that in the subsequent paragraphs I can give you little taste of that adventure too. 
\newpage
\section{The Richness of Sound}
We begin with a picture - specifically Figure 1. This is nothing more than a picture of the simplest wave possible. If you've ever plucked a guitar string you'll know that sound is the result of vibrations. When some object like a string, or a speaker, or your very own vocal chords vibrate that vibration translates into waves in the air which we then hear as sound. Given that all sound is composed of waves like these, we can begin our journey in trying to differentiate different sound sources by understanding what makes one wave different from the next. And an excellent way to do this is to look to the richest source of sound that our ears already find intelligible - music. 

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/a_basic_wave.pdf}}
\caption{\label{fig:my-label} A Simple Wave}
\end{figure}

\subsection{Blue Whales Yelling - Amplitude}
What makes music such a great place to start is the fact that the human mind is already so used to picking out different sections, rhythms, and instruments - an activity quite similar to what we want to do with our frog chorus. When you go to a concert you can distinguish the vocals from the percussion, and the harmony from the melody. But, depending on the kind of concert it can be much more difficult picking out what your friends are trying to say when you're already having to yell to hear yourself. And by the end you'll all be speaking in muted whispers no matter how loud you try to speak because you'll have lost your voices half way through the performance. This contrast, conveniently, is our first and simplest discriminator of sound - volume. As you raise your voice the volume of the sound that you are producing increases. The fact that you can barely hear each other speaks to the enormous volume of sound coming from the stage. So how is this represented in our wave? Volume is determined by the height of our wave - the farther the undulation has to go between each peak and valley the louder the sound is. This height is referred to as the \textbf{amplitude} and to get a sense of just how variable amplitude can be let's bring a blue whale to the concert.

Blue whales are the largest animals to have ever lived. Rather unsurprisingly then, they can be rather loud. How loud? Well let's just say that blue whales would have no trouble hearing each other at a rock concert. In fact they may instead draw the ire of their fellow concert goers as they drown out the music with their voices. The typical sound level at a rock concert is around 110 decibels (dB) \cite{chris}. Blue whales on the other hand have been recorded at up to 180 dB \cite{zcormier}. This difference is actually far more ridiculous than it first seems because of what a decibel actually is. When a decibel measurement increases by 10 it means the sound is around 3 \textit{times} louder. All to say that if our blue whales raised their voices, their chatter could get up to 21 times louder than the rock concert! Quite rude. Graphing this out in Figure 2 we can see that the amplitude of the blue whales yelling at one another (dashed line) is so large in comparison to the amplitude of the rock concert (solid line) that the rock concert's sound barely looks like a wave at all. Pretty incredible.

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/blue_whale_comparison.pdf}}
\caption{\label{fig:my-label} Blue Whale v. Rock Concert}
\end{figure}

\subsection{To Tune or Not to Tune - Pitch}
Okay, so amplitude is one way to distinguish one wave from another - what's up next? Well let's stop worrying about hearing our friends for a moment and turn back to the music. Remember how we said it's pretty easy for our brains to distinguish the vocalists from, say, the bass? Well there's a good reason for that - pitch. Pitch is how high or low a sound sounds. When that bass is rumbling all through your body you're experiencing a sound with a very low pitch. On the other hand when the singer hits that high note with a full crescendo that's a sound with an exhilarating high pitch. How does pitch show up in our wave? Well in contrast to amplitude - which was how high our waves got - pitch is all about how wide they are. The distance from one peak to the next is called the \textbf{wavelength} and the larger that distance is the lower the pitch of the sound. Pitch however is rarely measured in wavelengths. Instead the standard measurement used is something called Hertz (Hz). Hertz is a unit of \textbf{frequency} and frequency is simply the number of complete oscillations (peaks and troughs) per unit time. So, for example, when you hear that orchestras tune to A440 that 440 is specifically 440 Hz. Frequency and wavelength are interchangeable and tied together by a simple formula. Using the fact that we know the speed of sound $c$, if $w$ is the wavelength and $k$ is the frequency then the conversion is simply:

\begin{equation}
w = c/k
\end{equation}

So for example, if the speed of sound is 343 meters per second (m/s) then the wavelength of A440 is $343/440\approx0.78m$ or just over 2.5 feet. In comparison the highest note on a typical piano is at 7900 Hz \cite{wikipiano} which corresponds to a \textit{much} smaller wavelength ($343/7900=0.04m$). Figure 3 shows a comparison between the two with the solid line being A440 and the dashed line (which has a very very quick wiggle) being the highest note on a typical piano. Indeed the higher note has such a small wavelength it's becoming hard to see that it's a wave at all.

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/a440.pdf}}
\caption{\label{fig:my-label} A440 v. The Highest Note on a Piano}
\end{figure}

\subsection{The Quality of Sound - Superposition}
Now while pitch can certainly explain how we can tell the bass and the vocalist apart, how about sounds that are in the same range of pitch? For example a piano and our vocalist can both end up in in the same range of pitches and yet you'll still be able to tell them apart. What's going on here? Well, so far we've been oversimplifying things a lot. Remember how I said our picture of wave was the simplest picture possible? Well one of the ways in which it's super simple is that it's only got a single wavelength in it. In other words it has a single tone. Most sounds are not like this and are instead the composition of many wavelengths. This layering of wavelengths is called \textit{superposition} and it's best illustrated by an example. 

All sounds "begin" with a fundamental frequency - the lowest pitch in the sound. So let's begin with our simplest wave as the fundamental frequency (Fig 4.).

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/super1.pdf}}
\caption{\label{fig:my-label} Fundamental Frequency}
\end{figure}

With our fundamental frequency in place, let's add in our first higher frequency. Note that when superimposing waves, not only can their frequencies be different, but their amplitudes can be different as well (and almost always are). So to illustrate this we'll superimpose a wave with half the wavelength and half the amplitude on top of our fundamental frequency. Figure 5 shows both the two base waves (dashed) and the resulting superimposed wave (solid).

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/super2.pdf}}
\caption{\label{fig:my-label} Super Imposing Two Waves}
\end{figure}

Pretty wild right? (I find these kinds of waves starting looking really interesting and honestly kind of beautiful) 

Alright, let's add one more frequency in, this time with one quarter the wavelength and equal amplitude in comparison to our fundamental frequency (Fig. 6). 

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/super3.pdf}}
\caption{\label{fig:my-label} Three Frequencies}
\end{figure}

Alright, while this is cool and nerdy and all, how does this relate back to distinguishing a piano from a vocalist? Well, when you play a note on any instrument the resulting sound is the superposition of whole load of different frequencies grounded on top of that fundamental frequency. What makes each instrument sound different, even when they're playing similar notes, is the fact that their particular mix of frequencies and amplitudes are each different. The combination particular to one instrument is what gives it its particular sound. In fact, for a trained ear, these differences can even allow you to identify one instrument from another. For example a piano with higher amplitudes at higher frequencies literally sounds brighter than one with muted amplitudes at higher frequencies. Superposition then, and the particular mix of frequencies and amplitudes that make up a specific sound, is yet another tool in our tool box of ways to distinguish one sound source from another. 

\subsection{Vanishing Act - Phase}
So we've now got amplitude, wavelength, and superposition which brings us to the final component we can use in describing our waves. This one is probably the most abstract and weird of the bunch, but it's one we're going to take advantage of a \textit{lot} and its name is \textbf{phase}. 

Thus far we've been looking at all of these waves as being in one spot in our graph. But there's no reason why we can't start shifting them from left to right (Fig 7.). This shifting is phase.

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/phase.pdf}}
\caption{\label{fig:my-label} Two Waves with Different Phases}
\end{figure}

What gets really weird (and where we start to see the power of phase) is when we superimpose two waves with the same wavelength but different phases. 

Figure 8 shows the super position of the two waves from Figure 7. Note how because the waves are nearly in sync (peaks match with peaks and troughs match with troughs) they add together to create a wave with much higher amplitude. This is known as \textbf{constructive interference}.

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/phasesuper1.pdf}}
\caption{\label{fig:my-label} Constructive Interference}
\end{figure}

On the other hand, Figure 9 shows the superposition of two waves that are nearly out of sync (peaks at troughs and troughs at peaks). Note how in this case the resulting wave is much smaller than either of the constituents - this is \textbf{destructive interference}.

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/phasesuper2.pdf}}
\caption{\label{fig:my-label} Destructive Interference}
\end{figure}


What's really weird is that if you get the two waves to be exactly out of sync, the resulting superposition \textit{vanishes}! What does this mean physically? It means that the sound itself disappears! Yep, that's right, if two sound sources have just the right phase difference you'll suddenly no longer hear them even though the underlying sounds are there. Absolutely wild right? Well it turns out that this odd mathematical feature helps your brain figure out where sound is coming from.

Suppose a sound is coming directly from your right. That sound will obviously hit your right ear first. A very very short time later the same wave will hit your left ear, but at that point the undulation at your right ear will have changed (because the wave is constantly, well, waving). This difference in what your right and left ear are receiving is equivalent to the phase shift we were just talking about. Now suppose the sound is coming from directly in front of you. In this case the sound hits both ears at the same time because the distance to each ear is the same. This means there is no phase difference. What these two examples show is that as a sound source moves around your head the phase difference between your two ears changes. And your brain can use these differences to help you pick out where the sound is coming from! \cite{wikilocalization}. Pretty amazing, right? This happens to be the simplest version of a very cool technology called a \textbf{phased array} - something we'll be diving into detail next. But before we get to that we need to pull together all the tools we've gathered up so far and formalize them mathematically because phased arrays get pretty technical. So let's step back and do just that.

\subsection{Sums and Summaries - Mathematical Formalization}
Alright, so we've got all our pieces:
\begin{itemize}
\item \textbf{Amplitude:} The height of a wave, representing volume.
\item \textbf{Wavelength (or Frequency):} The width of a wave, representing pitch.
\item \textbf{Superposition:} The particular mix of amplitudes and frequencies that make up a sound.
\item \textbf{Phase:} An abstract sense of the "position" of a wave that leads to destructive or constructive interference.
\end{itemize}

How do we tie them all together mathematically? Well our simplest of waves is described by the following formula:
\begin{equation}
y=a e^{i\psi}e^{ikx}
\end{equation}
$e=2.71828$ is a mathematical constant known as Euler's number. $i=\sqrt{-1}$ is the imaginary number. $a$ is our amplitude. $k=2\pi/w$ where $w$ is our wavelength. Last but certainly not least, $\psi$ (pronounced like \textit{sigh}) is our phase. 

That then gets us three out of our four. So what about superposition? Well remember superposition is just adding many simple waves together, so we can do just that: 
\begin{equation}
y = a_1 e^{i\psi_1}e^{ik_1x} + a_2 e^{i\psi_2}e^{ik_2x} + ... + a_N e^{i\psi_N}e^{ik_Nx}
\end{equation}
Now writing out all these terms all of the time is going to get really burdensome, so we're going to take advantage of a little bit of mathematical notation that you may or may not be familiar with - the sum $\Sigma$. 

If you're not familiar with this notation, let's demonstrate with a simpler example. Suppose that you were adding the numbers 1 through 100. Without $\Sigma$ you would write:
\begin{equation}
1 + 2 + 3 + ... + 100
\end{equation}
With $\Sigma$ this same expression becomes:
\begin{equation}
\sum_{n=1}^{100}n
\end{equation}
which reads as - "add together all the $n$ (the thing to the right of the $\Sigma$) where $n$ starts at 1 (expression below the $\Sigma$) and goes all the way to 100 (number above the $\Sigma$)". I appreciate that this is probably pretty abstract and a little mind bending if this is your first time seeing it, but as we dive further into our little adventure you'll see just how useful this one bit of notation is. Alright, back to our waves.

Our superposition of waves goes from looking like this
\begin{equation}
y = a_1 e^{i\psi_1}e^{ik_1x} + a_2 e^{i\psi_2}e^{ik_2x} + ... + a_N e^{i\psi_N}e^{ik_Nx}
\end{equation}
to this:
\begin{equation}
y = \sum_{n=1}^{N}a_n e^{i\psi_n}e^{ik_nx}
\end{equation}
which is far neater and, as will become clear later, far easier to work with. 

That's it then! We've got our equation for a wave and are clear on the various components that make one sound different from another. With these tools in hand let's go and look at an absolutely incredible (and mathematically beautiful) technology for sound localization based on nothing more than our two ears - the phased array. 

\newpage
\section{To Phase Array}
\subsection{Back at the Pond}
With a firmer understanding of the components of sound, let's return to the pond.  Frogs in full chorus are a tricky bunch because while different species of frog may sound quite different, the individuals within a species are much more like instruments in the same section - while we find it easy to distinguish the violins from the cellos, unless someone is playing terribly off tune, telling the different cellos apart is extremely difficult if not impossible. This is because they've all got roughly the same pitch, are playing at nearly the same time, and have similar if not identical volumes. Frogs, surprisingly are quite the same. The song for a specific species is very distinct, in chorus they tend to overlap with one another, and in the competition to be heard everyone gets real loud. To complicate things even further they'll vary the length or repeats in their songs so we can't even try to identify a specific phrase as an individual. So then what are we to do if frequency and amplitude are so unhelpful to us at this specific stage (they'll actually come in handy later)? 

Well what we do know is that each frog is going to be calling from a specific spot - so while they may overlap in terms of a lot of things, position is unlikely to be one of them. This brings us straight back to our two ears. Recall from the last section how because of differences in when the sound hits each ear we get phase differences and that that difference in timing depends upon the location of the sound? Well this means that if you have multiple listening devices (in this case your two ears), the phase differences between those sources is a function of the position of the sound source. How does this help us? Well remember that phase differences create destructive and constructive interference - so if the phase difference is a function of location so too is whether the interference is constructive or destructive. 

Let's take an example (and note this is an illustrative example, the extent to which your brain actually does this is up for debate). Suppose that we have two sound sources. One directly in front of you and one directly to the right. For the source directly in front of you, the distance from the source to each of your ears will be exactly the same. Therefore the phase difference will be zero and as we saw in Section 2.4 this will mean perfect constructive interference - the combined wave will have twice the amplitude. Now consider the sound source at your right. Obviously the sound from that source will hit your right ear first and then your left. Now suppose the wavelength of this wave is such that when the sound hits your left ear it is at a peak at that ear and at a trough at your right ear - i.e. they are completely out of sync. Well then in this case we'll get perfect destructive interference and the superimposed sound will vanish. Now note this doesn't mean you'll just stop hearing the sound because your brain can do all sorts of funky things with the sounds coming to your ears; but, if we imagined two microphones reading in these sounds and superimposing them it would hear the sound in front as twice as loud and the sound to the right not at all which means we'd have a way of listening to one location while completely ignoring another which is exactly what we want!

Let's take this a step further. Returning two our digital ears (the microphones) we know that we can postprocess the data from each microphone however we like. In other words we can delay the signals coming from either microphone before combining them. So, for example we could delay the signal at the right microphone so that we get the two signals from the right sound source to line up and experience constructive interference. But this will of course cause the sound source at the front to go out of phase and vanish. Using digital post processing we've switched which location we're listening too! And because it's digital, we can do both the delay and no delay at the same time which means we can isolate each sound source and listen to them simultaneously! 

So now imagine that we had a setup like this at the pond. In theory it may be possible for us to vary the delays on our digital ears in such a way that we could isolate many different locations on the pond surface simultaneously and listen to them all at once. If we can get the size of those isolated blocks to be small enough then we could listen to the individual frogs and have broken the frog cipher! 

This setup with the multiple microphones and the time delays is known as a \textbf{phased array} and is a technology used quite a bit in communications. As will become clear soon phased arrays require a great deal of precision and far more than two digital ears. So in order to figure out how this would work in reality we're going to need to go through the mathematics of phased array antennas which is what the rest of this section is all about. Then in the following section we'll be able to return to the question of what it would take to build a phased array antenna that could listen to individual frogs on a pond simultaneously. 
\subsection{Building an Alien}
So before we dig in it's time to give credit where credit is due. The following derivations and treatment come almost entirely from an absolutely incredible book - \textit{Phased Array Antenna Handbook} \cite{phasedhandbook}. 

We begin with a large grid, sitting as all things mathematical tend to do, on a plane (a big, perfectly flat surface). This grid is composed of lovely, omni-directional antennas. Specifically they are layered $n$ deep along the $y$-axis and $m$ deep along the $x$-axis which means we have $m\times n$ antennas in total. This setup is known as a planar phased array antenna. Next we denote the position of our sound source by two angles (illustrated in Figure 10) $\theta$ and $\phi$. 

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/phased_array_diagram.pdf}}
\caption{\label{fig:my-label} A Planar Phased Array}
\end{figure}


Let us designate the distance from our sound source to the $j$th element as $R_j$. Then we know from our wave equation that for a single frequency in the sound source's superposition that at our $j$th antenna element we have:

\begin{equation}
a e^{ikR_j}
\end{equation}

Right off the bat we're making a few simplifications here. First, in reality as you get farther from a sound source the amplitude shrinks but we're just assuming we're only interested in the amplitude near the antenna $a$ so we don't need to account for this reduction in sound level. Second we're implicitly making what's called a \textit{far field} assumption here - namely that the source is far enough away that we can use our simple wave equation from the last section (if the sound source is very close the equation gets far more complicated). We'll get into what this means for our antenna design in the next section, but I figured it would be useful to mention now. 

Now if we designate the distance from the sound source to the center of our antenna as $R$, designate $\mathbf{\hat{r}}$ as the unit vector (vector of length one) in the direction of our sound source, and $\mathbf{r_j}$ as the vector from our antenna's center to the $j$th element, then:

\begin{equation}
R_j \approx R - \mathbf{\hat{r}}\bullet\mathbf{r_j}
\end{equation}

Which means that we have

\begin{equation}
a e^{ik(R - \mathbf{\hat{r}}\bullet\mathbf{r_j})} = ae^{ikR}e^{-ik\mathbf{\hat{r}}\bullet\mathbf{r_j}}
\end{equation}

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/R_approx.pdf}}
\caption{\label{fig:my-label} Approximation of $R$}
\end{figure}

Now because we're only interested in the difference in phases (and not their sign) all we care about is:

\begin{equation}
ae^{ik\mathbf{\hat{r}}\bullet\mathbf{r_j}}
\end{equation}

If we use Figure x to work out what these vectors are in terms of the coordinate vectors $\mathbf{\hat{x}}$, $\mathbf{\hat{y}}$, and $\mathbf{\hat{z}}$ we have:

\begin{equation}
\mathbf{r_j} = \mathbf{\hat{x}}x_j + \mathbf{\hat{y}}y_j + \mathbf{\hat{z}}z_j
\end{equation}

\begin{equation}
\mathbf{\hat{r}} = \mathbf{\hat{x}}u + \mathbf{\hat{y}}v + \mathbf{\hat{z}}\cos \theta
\end{equation}

where $u=\sin \theta \cos \phi$ and $v=\sin \theta \sin \phi$.

Luckily, because we know things are laid out in a grid in the $x,y$ then if $p,q$ are how far in the $j$th antenna element are along $x$ and $y$ respectively then we know that:

\begin{equation}
 \mathbf{\hat{x}}x_j + \mathbf{\hat{y}}y_j + \mathbf{\hat{z}}z_j =  \mathbf{\hat{x}}qd_x + \mathbf{\hat{y}}pd_y
\end{equation}

and therefore that:

\begin{equation}
\mathbf{\hat{r}}\bullet\mathbf{r_j}=qd_xu+pd_yv
\end{equation}

so that

\begin{equation}
ae^{ik\mathbf{\hat{r}}\bullet\mathbf{r_j}}=ae^{ikqd_xu}e^{ikpd_yv}
\end{equation}

Alright, thus far we've only been talking about what the microphone sees without modifications. But remember that the whole point here is going to be to introduce phase shifts in order to create the constructive or destructive interference that we want. A phase shift is simply represented by adding (or subtracting) a value from our exponent:

\begin{equation}
ae^{ikqd_xu}e^{ikpd_yv}e^{i\psi}
\end{equation}

If we rewrite $\psi$ as $\psi = -k(qd_xu_0+pd_yv_0)$ then we have:
\begin{equation}
ae^{ikqd_xu}e^{ikpd_yv}e^{i\psi}=ae^{ikqd_xu}e^{ikpd_yv}e^{-ik(qd_xu_0+pd_yv_0)}
\end{equation}
\begin{equation}
ae^{ikqd_xu}e^{ikpd_yv}e^{-ik(qu_0+pv_0)}= ae^{ikqd_x(u-u_0)}e^{ikpd_y(v-v_0)}
\end{equation}

Why is this form useful? Well we can see that if $u_0=u$ and $v_0=v$ then the exponential terms become $1$ (as we're raising to the power of 0) regardless of which antenna in our array we're talking about. In other words we have complete constructive interference! So, if we want to listen to the direction defined by $\theta$ and $\phi$ we simply need to set a phase shift on each of our antennas that is equal to $\psi = -k(qd_xu+pd_yv)$.

Alright just to wrap things up here what does the full formula for the entire impact to our full antenna look like? Well we just need to sum up the contributions from each antenna:

\begin{equation}
I=\sum_{p=1}^n \sum_{q=1}^m  ae^{ikqd_x(u-u_0)}e^{ikpd_y(v-v_0)}
\end{equation}



\subsection{Enough Math, Show Me Something}
We've now got a mathematical formalization for our phased array but what does this actually look like? Well let's take a simple example of a single row of antennas (i.e. $m=1$) and graph things out!

Now for those of you who have been playing close attention you'll have noticed that our $I$ from the preceding subsection is an imaginary number. Obviously we won't be plotting anything imaginary so what are we going to be plotting? We'll be plotting the power as a function of $\theta$ (the source location). Power is defined as:

\begin{equation}
P=I\bar{I}
\end{equation}

or as $I$ multiplied by it's conjugate. 

As a final note before we get plotting we're going to be plotting the power in $dB$ relative to the maximum power. That just means we're going to be using a log scale rather than a linear one. Alright let's get to plotting.

We'll start with the simplest case where $u_0$ (indicative of the \textbf{steering angle}) is steered to $\theta=0$. We'll include 10 elements in our array and space them half a wavelength apart (more on this in the next section). 

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/steer0_el10.pdf}}
\caption{\label{fig:my-label} Steering Angle $\theta=0$ with 10 Element Array}
\end{figure}

Right away there's several things to take away from this graph. First while we have steered toward $\theta=0$ and are getting our most constructive interference at that angle, it's not as if the constructive interference just falls off immediately as we move away from $\theta=0$. Instead we see that we have this whole area around $\theta=0$ where we still have considerable power before it drops off sharply. That area is known as the \textbf{beam} and its width (measured in various ways) is the \textbf{beam width}. Put another way, we won't just be hearing things at $\theta=0$ but also things throughout the beam. How narrow our beam is (and thus how small the beam width) determines how localized our listening is. 

Second is the fact that the in the graph we see these steep drop offs followed by subsequent peaks. These other peaks are known as \textbf{sidelobes} and are other areas in which we'll get some non-negligible signal from. 

Let's now look at a couple other examples. First let's see the effect of changing the steering angle by modifying our $u_0$. 

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/steerpi6_el10.pdf}}
\caption{\label{fig:my-label} Steering Angle $\theta=\pi/6$ with 10 Element Array}
\end{figure}

This is exactly as we'd expect, as the steering angle changes the position of our beam should change as well. 

Finally let's look at what happens when we increase the number of elements to 20.

\begin{figure}[!htb]
\center{\includegraphics[width=\textwidth]
{figures/steer0_el20.pdf}}
\caption{\label{fig:my-label} Steering Angle $\theta=0$ with 20 Element Array}
\end{figure}

Notice how in comparison to our 10 element array our beam width has shrunk dramatically. As we add elements our localization improves and where we receive signal from shrinks. This is just one aspect of antenna design which we shall turn to next.

\newpage


\section{How Tall is A-flat?}
The one where we show how difficult sound is to work with and go over antenna design

\subsection{Far A Field}
\section{The Fourth Dimension}
The one where we explain how we're going to get around sound's limitations
\section{A Frog in a Sound-stack}
The one where we explain the approach to the unsolved problem
\section{The Descent of Math}
The one where we work out the ML math
\section{Divergent Degenerates}
The one where we deal with some technical issues of our gradient
\section{We Did a Thing}
The one where we bring it all together
\section{What's Going On?}
The one where we reflect on what we achieved
\newpage
\bibliographystyle{plain}
\bibliography{reference}

\end{document}